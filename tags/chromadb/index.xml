<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ChromaDB on Zoltán's Blog</title><link>https://bzoltan1.github.io/tags/chromadb/</link><description>Recent content in ChromaDB on Zoltán's Blog</description><generator>Hugo</generator><language>en-us</language><managingEditor>zoltan.balogh@suse.com (Zoltán Balogh)</managingEditor><webMaster>zoltan.balogh@suse.com (Zoltán Balogh)</webMaster><copyright>© Zoltán Balogh 2021</copyright><lastBuildDate>Fri, 23 May 2025 10:00:00 +0200</lastBuildDate><atom:link href="https://bzoltan1.github.io/tags/chromadb/index.xml" rel="self" type="application/rss+xml"/><item><title>Building a Local Bugzilla RAG System</title><link>https://bzoltan1.github.io/building-a-local-bugzilla-rag-system/</link><pubDate>Fri, 23 May 2025 10:00:00 +0200</pubDate><author>zoltan.balogh@suse.com (Zoltán Balogh)</author><guid>https://bzoltan1.github.io/building-a-local-bugzilla-rag-system/</guid><description>&lt;p>My goal was to build a local database that could:&lt;/p>
&lt;ul>
&lt;li>Ingest my ~4GB Bugzilla database&lt;/li>
&lt;li>Answer questions or give advice on new bugs based on historical ones&lt;/li>
&lt;li>Run offline on my openSUSE Tumbleweed machine, which is equipped with 64GB RAM and an AMD Ryzen 7 PRO 7840U&lt;/li>
&lt;/ul>
&lt;p>Naturally, my first idea was to build a standalone LLM like GPT. But fine-tuning an LLM on custom data is resource-intensive—a massive understatement. When I started to fine-tune an LLM on my laptop, I let the process run for a full week, and it reached only 1%. Using cloud-based services or investing in powerful new hardware were not options. Also, the problem with standalone LLMs is that they may hallucinate or generate inaccurate information, especially on domain-specific topics. The other disadvantage of using LLMs is that they are static; once trained, they don&amp;rsquo;t know anything that happened afterward.&lt;/p></description></item></channel></rss>